import os
import streamlit as st
import pandas as pd
import requests
import base64
import hashlib
from io import BytesIO
from datetime import datetime, timedelta
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment


GITHUB_TOKEN = st.secrets["GITHUB_TOKEN"]  # åœ¨ Streamlit Cloud ç”¨ secrets
REPO_NAME = "TTTriste06/semi"
BRANCH = "main"

CONFIG = {
    "output_file": f"è¿è¥æ•°æ®è®¢å•-åœ¨åˆ¶-åº“å­˜æ±‡æ€»æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
    "selected_month": None,
    "pivot_config": {
        "èµ›å“-æœªäº¤è®¢å•.xlsx": {
            "index": ["æ™¶åœ†å“å", "è§„æ ¼", "å“å"],
            "columns": "é¢„äº¤è´§æ—¥",
            "values": ["è®¢å•æ•°é‡", "æœªäº¤è®¢å•æ•°é‡"],
            "aggfunc": "sum",
            "date_format": "%Y-%m"
        },
        "èµ›å“-æˆå“åœ¨åˆ¶.xlsx": {
            "index": ["å·¥ä½œä¸­å¿ƒ", "å°è£…å½¢å¼", "æ™¶åœ†å‹å·", "äº§å“è§„æ ¼", "äº§å“å“å"],
            "columns": "é¢„è®¡å®Œå·¥æ—¥æœŸ",
            "values": ["æœªäº¤"],
            "aggfunc": "sum",
            "date_format": "%Y-%m"
        },
        "èµ›å“-CPåœ¨åˆ¶.xlsx": {
            "index": ["æ™¶åœ†å‹å·", "äº§å“å“å"],
            "columns": "é¢„è®¡å®Œå·¥æ—¥æœŸ",
            "values": ["æœªäº¤"],
            "aggfunc": "sum",
            "date_format": "%Y-%m"
        },
        "èµ›å“-æˆå“åº“å­˜.xlsx": {
            "index": ["WAFERå“å", "è§„æ ¼", "å“å"],
            "columns": "ä»“åº“åç§°",
            "values": ["æ•°é‡"],
            "aggfunc": "sum"
        },
        "èµ›å“-æ™¶åœ†åº“å­˜.xlsx": {
            "index": ["WAFERå“å", "è§„æ ¼"],
            "columns": "ä»“åº“åç§°",
            "values": ["æ•°é‡"],
            "aggfunc": "sum"
        }
    }
}

def upload_to_github(file, path_in_repo, commit_message):
    api_url = f"https://api.github.com/repos/{REPO_NAME}/contents/{path_in_repo}"
    
    file.seek(0)  # ç¡®ä¿æŒ‡é’ˆåœ¨å¼€å¤´
    file_content = file.read()
    encoded_content = base64.b64encode(file_content).decode('utf-8')

    # å…ˆè·å–æ–‡ä»¶ SHAï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    response = requests.get(api_url, headers={"Authorization": f"token {GITHUB_TOKEN}"})
    if response.status_code == 200:
        sha = response.json()['sha']
    else:
        sha = None

    # æ„é€  payload
    payload = {
        "message": commit_message,
        "content": encoded_content,
        "branch": BRANCH
    }
    if sha:
        payload["sha"] = sha

    # ä¸Šä¼ æ–‡ä»¶
    response = requests.put(api_url, json=payload, headers={"Authorization": f"token {GITHUB_TOKEN}"})

    # ç»“æœåé¦ˆ
    if response.status_code in [200, 201]:
        st.success(f"{path_in_repo} ä¸Šä¼ æˆåŠŸï¼")
    else:
        st.error(f"ä¸Šä¼ å¤±è´¥: {response.json()}")


def preprocess_mapping_file(df):
    # åªå–å‰6åˆ—
    df = df.iloc[:, :6]
    # é‡å‘½ååˆ—
    df.columns = ['æ—§è§„æ ¼', 'æ—§å“å', 'æ—§æ™¶åœ†å“å', 'æ–°è§„æ ¼', 'æ–°å“å', 'æ–°æ™¶åœ†å“å']
    return df

def download_mapping_from_github(path_in_repo):
    api_url = f"https://api.github.com/repos/{REPO_NAME}/contents/{path_in_repo}"
    response = requests.get(api_url, headers={
        "Authorization": f"token {GITHUB_TOKEN}"
    })
    if response.status_code == 200:
        content = base64.b64decode(response.json()['content'])
        df = pd.read_excel(pd.io.common.BytesIO(content))
        if df.shape[1] >= 6:
            df = preprocess_mapping_file(df)
        else:
            st.warning(f"mapping_file.xlsx åˆ—æ•°ä¸è¶³ï¼šåªå‘ç° {df.shape[1]} åˆ—ï¼Œéœ€è¦è‡³å°‘ 6 åˆ—")
            df = pd.DataFrame(columns=['æ—§è§„æ ¼', 'æ—§å“å', 'æ—§æ™¶åœ†å“å', 'æ–°è§„æ ¼', 'æ–°å“å', 'æ–°æ™¶åœ†å“å'])
        return df
    else:
        st.warning("GitHub ä¸Šæ‰¾ä¸åˆ° mapping_file.xlsxï¼Œç”¨é»˜è®¤è¡¨æˆ–è¯·å…ˆä¸Šä¼ ")
        return pd.DataFrame(columns=['æ—§è§„æ ¼', 'æ—§å“å', 'æ—§æ™¶åœ†å“å', 'æ–°è§„æ ¼', 'æ–°å“å', 'æ–°æ™¶åœ†å“å'])

def download_excel_from_github(url, token=None):
    headers = {"Authorization": f"token {token}"} if token else {}
    response = requests.get(url, headers=headers)
    content_type = response.headers.get('Content-Type', '')

    # æ£€æŸ¥æ–‡ä»¶æ˜¯ä¸æ˜¯ Excel
    if 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' not in content_type:
        raise ValueError("ä¸‹è½½çš„ä¸æ˜¯ Excel æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ GitHub é“¾æ¥æˆ–æƒé™")

    return pd.read_excel(BytesIO(response.content))

import pandas as pd
import requests
from io import BytesIO

def download_backup_file(file_name):
    api_url = f"https://api.github.com/repos/{REPO_NAME}/contents/{file_name}"
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}
    response = requests.get(api_url, headers=headers)

    if response.status_code != 200:
        st.warning(f"âš ï¸ æ— æ³•ä¸‹è½½ {file_name}ï¼ŒGitHub è¿”å›ç  {response.status_code}")
        return pd.DataFrame()  # è¿”å›ç©º DataFrameï¼Œä¿è¯ä¸»ç¨‹åºä¸å´©æºƒ

    content = response.json().get('content')
    if not content:
        st.warning(f"âš ï¸ {file_name} æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–è§£æå¤±è´¥")
        return pd.DataFrame()

    file_bytes = BytesIO(base64.b64decode(content))

    try:
        df = pd.read_excel(file_bytes)
    except Exception as e:
        st.warning(f"âš ï¸ {file_name} è§£æ Excel å¤±è´¥ï¼š{e}ï¼Œå°†åˆ›å»ºç©º sheetã€‚")
        return pd.DataFrame()

    return df
        
def process_date_column(df, date_col, date_format):
    if pd.api.types.is_numeric_dtype(df[date_col]):
        df[date_col] = df[date_col].apply(lambda x: datetime(1899, 12, 30) + timedelta(days=float(x)))
    else:
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
    df[f'{date_col}_å¹´æœˆ'] = df[date_col].dt.strftime(date_format)
    return df

def apply_mapping_and_merge(df, mapping_df):
    mapping_df = mapping_df.dropna()
    
    df = df.merge(
        mapping_df,
        how='left',
        left_on=['æ™¶åœ†å“å', 'è§„æ ¼', 'å“å'],
        right_on=['æ—§æ™¶åœ†å“å', 'æ—§è§„æ ¼', 'æ—§å“å']
    )
    
    df['æ™¶åœ†å“å'] = df['æ–°æ™¶åœ†å“å'].combine_first(df['æ™¶åœ†å“å'])
    df['è§„æ ¼'] = df['æ–°è§„æ ¼'].combine_first(df['è§„æ ¼'])
    df['å“å'] = df['æ–°å“å'].combine_first(df['å“å'])
    
    df.drop(columns=['æ—§æ™¶åœ†å“å', 'æ—§è§„æ ¼', 'æ—§å“å', 'æ–°æ™¶åœ†å“å', 'æ–°è§„æ ¼', 'æ–°å“å'], inplace=True)
    
    group_cols = [col for col in df.columns if col not in df.select_dtypes(include='number').columns]
    agg_cols = df.select_dtypes(include='number').columns.tolist()
    df_merged = df.groupby(group_cols, as_index=False)[agg_cols].sum()
    
    return df_merged

def create_pivot(df, config, filename, mapping_df=None):
    if 'date_format' in config:
        config = config.copy()
        config['columns'] = f"{config['columns']}_å¹´æœˆ"
    pivoted = pd.pivot_table(df, index=config['index'], columns=config['columns'], values=config['values'],
                             aggfunc=config['aggfunc'], fill_value=0)
    pivoted.columns = [f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in pivoted.columns]
    pivoted = pivoted.reset_index()

    if mapping_df is not None and filename == "èµ›å“-æœªäº¤è®¢å•.xlsx":
        pivoted = apply_mapping_and_merge(pivoted, mapping_df)

    if CONFIG['selected_month'] and filename == "èµ›å“-æœªäº¤è®¢å•.xlsx":
        history_cols = [col for col in pivoted.columns if '_' in col and col.split('_')[-1][:4].isdigit() and col.split('_')[-1] < CONFIG['selected_month']]
        history_order_cols = [col for col in history_cols if 'è®¢å•æ•°é‡' in col and 'æœªäº¤è®¢å•æ•°é‡' not in col]
        history_pending_cols = [col for col in history_cols if 'æœªäº¤è®¢å•æ•°é‡' in col]
        if history_order_cols:
            pivoted['å†å²è®¢å•æ•°é‡'] = pivoted[history_order_cols].sum(axis=1)
        if history_pending_cols:
            pivoted['å†å²æœªäº¤è®¢å•æ•°é‡'] = pivoted[history_pending_cols].sum(axis=1)
        pivoted.drop(columns=history_cols, inplace=True)
        fixed_cols = [col for col in pivoted.columns if col not in ['å†å²è®¢å•æ•°é‡', 'å†å²æœªäº¤è®¢å•æ•°é‡']]
        if 'å†å²è®¢å•æ•°é‡' in pivoted.columns:
            fixed_cols.insert(len(config['index']), 'å†å²è®¢å•æ•°é‡')
        if 'å†å²æœªäº¤è®¢å•æ•°é‡' in pivoted.columns:
            fixed_cols.insert(len(config['index']) + 1, 'å†å²æœªäº¤è®¢å•æ•°é‡')
        pivoted = pivoted[fixed_cols]

    return pivoted

def adjust_column_width(writer, sheet_name, df):
    worksheet = writer.sheets[sheet_name]
    for idx, col in enumerate(df.columns, 1):
        max_len = df[col].astype(str).str.len().max()
        header_len = len(str(col))
        width = max(max_len, header_len) * 1.2 + 5
        worksheet.column_dimensions[get_column_letter(idx)].width = min(width, 50)

def main():
    st.set_page_config(
        page_title='æˆ‘æ˜¯æ ‡é¢˜',
        page_icon=' ',
        layout='wide'
    )

    with st.sidebar:
        st.title("æ¬¢è¿æ¥åˆ°æˆ‘çš„åº”ç”¨")
        st.markdown('---')
        st.markdown('è¿™æ˜¯å®ƒçš„ç‰¹æ€§ï¼š\n- feature 1\n- feature 2\n- feature 3')

    st.title('Excel æ•°æ®å¤„ç†ä¸æ±‡æ€»å·¥å…·')
    selected_month = st.text_input('è¯·è¾“å…¥æˆªè‡³æœˆä»½ï¼ˆå¦‚ 2025-03ï¼Œå¯é€‰ï¼‰')
    CONFIG['selected_month'] = selected_month if selected_month else None

    uploaded_files = st.file_uploader('ä¸Šä¼  Excel æ–‡ä»¶ï¼ˆ5ä¸ªæ–‡ä»¶ï¼‰', type=['xlsx'], accept_multiple_files=True)
    pred_file = st.file_uploader('ä¸Šä¼ é¢„æµ‹æ–‡ä»¶', type=['xlsx'], key='pred_file')
    safety_file = st.file_uploader('ä¸Šä¼ å®‰å…¨åº“å­˜æ–‡ä»¶', type=['xlsx'], key='safety_file')
    mapping_file = st.file_uploader('ä¸Šä¼ æ–°æ—§æ–™å·æ–‡ä»¶', type=['xlsx'], key='mapping_file')

    # åŠ è½½ mapping_file DataFrame
    mapping_df = None
    if mapping_file:
        mapping_df = pd.read_excel(mapping_file)
        mapping_df = preprocess_mapping_file(mapping_df)

    if pred_file:
        upload_to_github(pred_file, "pred_file.xlsx", "ä¸Šä¼ é¢„æµ‹æ–‡ä»¶")
    if safety_file:
        upload_to_github(safety_file, "safety_file.xlsx", "ä¸Šä¼ å®‰å…¨åº“å­˜æ–‡ä»¶")
    if mapping_file:
        upload_to_github(mapping_file, "mapping_file.xlsx", "ä¸Šä¼ æ–°æ—§æ–™å·æ–‡ä»¶")

    if st.button('æäº¤å¹¶ç”ŸæˆæŠ¥å‘Š') and uploaded_files:
        with pd.ExcelWriter(CONFIG['output_file'], engine='openpyxl') as writer:
            # ç”¨äºå­˜å‚¨æœªäº¤è®¢å•çš„å‰ä¸‰åˆ—æ•°æ®
            unfulfilled_orders_summary = pd.DataFrame()
            df_safety = pd.DataFrame()

            for f in uploaded_files:
                filename = f.name
                if filename not in CONFIG['pivot_config']:
                    st.warning(f"è·³è¿‡æœªé…ç½®çš„æ–‡ä»¶: {filename}")
                    continue

                df = pd.read_excel(f)
                config = CONFIG['pivot_config'][filename]

                if 'date_format' in config and config['columns'] in df.columns:
                    df = process_date_column(df, config['columns'], config['date_format'])

                pivoted = create_pivot(df, config, filename, mapping_df)
                sheet_name = filename[:30].rstrip('.xlsx')
                pivoted.to_excel(writer, sheet_name=sheet_name, index=False)
                adjust_column_width(writer, sheet_name, pivoted)

                # ä¿å­˜æœªäº¤è®¢å•çš„å‰ä¸‰åˆ—ï¼ˆå»é‡ï¼‰
                if filename == "èµ›å“-æœªäº¤è®¢å•.xlsx":
                    cols_to_copy = [col for col in pivoted.columns if col in ["æ™¶åœ†å“å", "è§„æ ¼", "å“å"]]
                    unfulfilled_orders_summary = pivoted[cols_to_copy].drop_duplicates()

            # å†™å…¥å®‰å…¨åº“å­˜ sheet
            if safety_file:
                df_safety = pd.read_excel(safety_file)
            else:
                df_safety = download_backup_file("safety_file.xlsx")
            df_safety.to_excel(writer, sheet_name='èµ›å“-å®‰å…¨åº“å­˜', index=False)
            adjust_column_width(writer, 'èµ›å“-å®‰å…¨åº“å­˜', df_safety)

            # å†™å…¥é¢„æµ‹æ–‡ä»¶ sheet
            if pred_file:
                df_pred = pd.read_excel(pred_file)
            else:
                df_pred = download_backup_file("pred_file.xlsx")
            df_pred.to_excel(writer, sheet_name='èµ›å“-é¢„æµ‹', index=False)
            adjust_column_width(writer, 'èµ›å“-é¢„æµ‹', df_pred)

            # å†™å…¥æ–°æ—§æ–™å·æ–‡ä»¶ sheet
            if mapping_file:
                df_mapping = pd.read_excel(mapping_file)
            else:
                df_mapping = download_backup_file("mapping_file.xlsx")
            df_mapping.to_excel(writer, sheet_name='èµ›å“-æ–°æ—§æ–™å·', index=False)
            adjust_column_width(writer, 'èµ›å“-æ–°æ—§æ–™å·', df_mapping)

            # å†™å…¥æ±‡æ€» sheet
            if not unfulfilled_orders_summary.empty:
                st.write("âœ… unfulfilled_orders_summaryï¼ˆæœªäº¤è®¢å•æ±‡æ€»ï¼‰éç©ºï¼Œè¡Œæ•°ï¼š", len(unfulfilled_orders_summary))
            else:
                st.warning("âš ï¸ unfulfilled_orders_summary æ˜¯ç©ºçš„ï¼Œæ²¡æœ‰æœªäº¤è®¢å•æ±‡æ€»æ•°æ®ã€‚")
            
            if not df_safety.empty:
                st.write("âœ… df_safetyï¼ˆå®‰å…¨åº“å­˜è¡¨ï¼‰éç©ºï¼Œè¡Œæ•°ï¼š", len(df_safety))
            else:
                st.warning("âš ï¸ df_safety æ˜¯ç©ºçš„ï¼Œæ²¡æœ‰å®‰å…¨åº“å­˜æ•°æ®ã€‚")
            
            if not unfulfilled_orders_summary.empty and not df_safety.empty:
                # é‡å‘½åå®‰å…¨åº“å­˜åˆ—
                df_safety_renamed = df_safety.rename(columns={
                    'WaferID': 'æ™¶åœ†å“å',
                    'OrderInformation': 'è§„æ ¼',
                    'ProductionNO.': 'å“å'
                })
            
                # ç»Ÿä¸€åˆ—ä¸ºå­—ç¬¦ä¸²ã€å»ç©ºæ ¼
                for col in ['æ™¶åœ†å“å', 'è§„æ ¼', 'å“å']:
                    df_safety_renamed[col] = df_safety_renamed[col].astype(str).str.strip()
                    unfulfilled_orders_summary[col] = unfulfilled_orders_summary[col].astype(str).str.strip()
            
                # ä¿ç•™éœ€è¦çš„åˆ—
                df_safety_subset = df_safety_renamed[['æ™¶åœ†å“å', 'è§„æ ¼', 'å“å', 'InvWaf', 'InvPart']].copy()
            
                # åˆå¹¶ + å¸¦ indicator
                merged_df = pd.merge(
                    unfulfilled_orders_summary,
                    df_safety_subset,
                    on=['æ™¶åœ†å“å', 'è§„æ ¼', 'å“å'],
                    how='left',
                    indicator=True
                )
            
                # æ˜¾ç¤ºåŒ¹é…æƒ…å†µç»Ÿè®¡
                st.write("åŒ¹é…ç»Ÿè®¡ï¼š")
                st.write(merged_df['_merge'].value_counts())
            
                # æ˜¾ç¤ºæ²¡æœ‰åŒ¹é…æˆåŠŸçš„æ ·ä¾‹
                unmatched = merged_df[merged_df['_merge'] != 'both']
                if not unmatched.empty:
                    st.write("âŒ æ²¡æœ‰åŒ¹é…ä¸Šçš„æ ·ä¾‹ï¼ˆå‰10è¡Œï¼‰ï¼š")
                    st.write(unmatched[['æ™¶åœ†å“å', 'è§„æ ¼', 'å“å']].head(10))
                else:
                    st.write("ğŸ‰ å…¨éƒ¨æˆåŠŸåŒ¹é…ï¼")
            
                # å»æ‰ indicator åˆ—
                merged_df.drop(columns=['_merge'], inplace=True)
            
                # æ›´æ–°æ±‡æ€» DataFrame
                unfulfilled_orders_summary = merged_df


                

            
                # å†™å…¥ Excelï¼Œä»ç¬¬2è¡Œå¼€å§‹ï¼ˆç¬¬1è¡Œç©ºå‡ºæ¥ï¼‰
                unfulfilled_orders_summary.to_excel(writer, sheet_name='æ±‡æ€»', index=False, startrow=1)
                adjust_column_width(writer, 'æ±‡æ€»', unfulfilled_orders_summary)
            
                worksheet = writer.book['æ±‡æ€»']
            
                # åˆå¹¶ D1:E1ï¼ˆç¬¬4,5åˆ—çš„ç¬¬ä¸€è¡Œï¼‰å†™â€œå®‰å…¨åº“å­˜â€
                worksheet.merge_cells('D1:E1')
                worksheet['D1'] = 'å®‰å…¨åº“å­˜'
                worksheet['D1'].alignment = Alignment(horizontal='center', vertical='center')
            
                # ç¬¬äºŒè¡Œ D åˆ—ã€E åˆ—å†™æ ‡é¢˜
                worksheet['D2'] = 'InvWafï¼ˆç‰‡ï¼‰'
                worksheet['D2'].alignment = Alignment(horizontal='center', vertical='center')
                worksheet['E2'] = 'InvPart'
                worksheet['E2'].alignment = Alignment(horizontal='center', vertical='center')
            
                # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
                for idx, col in enumerate(worksheet.columns, 1):
                    col_letter = get_column_letter(idx)
                    max_length = 0
                    for cell in col:
                        try:
                            if cell.value:
                                cell_len = sum(2 if ord(char) > 127 else 1 for char in str(cell.value))
                                max_length = max(max_length, cell_len)
                        except:
                            pass
                    worksheet.column_dimensions[col_letter].width = max_length + 5


        # ä¸‹è½½æŒ‰é’®
        with open(CONFIG['output_file'], 'rb') as f:
            st.download_button('ä¸‹è½½æ±‡æ€»æŠ¥å‘Š', f, CONFIG['output_file'])



if __name__ == '__main__':
    main()
